{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Part I: Sentiment Analysis with a Twitter Dataset.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jennylin331431/JSC270_NLP_Project/blob/main/Part_I_Sentiment_Analysis_with_a_Twitter_Dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initial data import and cleaning"
      ],
      "metadata": {
        "id": "P0XWgNYXCjif"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "import statsmodels.api as sm\n",
        "import statsmodels.formula.api as smf\n",
        "import random\n",
        "import math\n",
        "import io"
      ],
      "metadata": {
        "id": "0_DmPPGkoMHk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbe135bc-3f96-4ab2-ba51-161573d81a07"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import data\n",
        "from google.colab import files\n",
        "test = files.upload()\n",
        "train= files.upload()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38,
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          }
        },
        "id": "19ltWTcfnW1M",
        "outputId": "da16f46c-432b-477e-a473-4d59c785c9c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a32cf39c-6872-42e3-bfd7-72e0df27bdf3\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a32cf39c-6872-42e3-bfd7-72e0df27bdf3\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = pd.read_csv(io.BytesIO(test['covid-tweets-test.csv']), sep = ',')\n",
        "train_data = pd.read_csv(io.BytesIO(train['covid-tweets-train.csv']), sep = ',')"
      ],
      "metadata": {
        "id": "oosvKM3Yqhez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check to see if Sentiment values are 0 and 1\n",
        "print(train_data.value_counts(\"Sentiment\"))\n",
        "print(test_data.value_counts(\"Sentiment\"))"
      ],
      "metadata": {
        "id": "DupRKj0ij-1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the Sentiment values that are not 0, 1, or 2\n",
        "train_data.drop(train_data.loc[(train_data['Sentiment']== ' PA\"') | (train_data['Sentiment'] == ' England\"')].index, inplace=True)"
      ],
      "metadata": {
        "id": "PMHi3pwasu6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop N/A values\n",
        "train_data.dropna(inplace=True)\n",
        "test_data.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "VxKLPwky3v9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert selected columns to floats\n",
        "train_data['Sentiment'] = train_data['Sentiment'].astype('float64')\n",
        "test_data['Sentiment'] = test_data['Sentiment'].astype('float64')"
      ],
      "metadata": {
        "id": "nIkURE6Tv3D0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check that there are no null values\n",
        "train_data.isnull().sum()"
      ],
      "metadata": {
        "id": "DQsNkDEV3ljz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.head()"
      ],
      "metadata": {
        "id": "e7Qutw-Kw81F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part (A)\n",
        "\n",
        "(1 pt) Consider the training data. What is the balance between the three classes? In other words, what proportion of the observations (in the training set) belong to each class?"
      ],
      "metadata": {
        "id": "W99KmabWCpLM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.value_counts('Sentiment')"
      ],
      "metadata": {
        "id": "L38QRD8wr0ks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get balance between three classes\n",
        "total =  18042 + 15397 + 7712\n",
        "print(\"Sentiment 0 proportion: \", 15397/total)\n",
        "print(\"Sentiment 1 proportion: \", 7712/total)\n",
        "print(\"Sentiment 2 proportion: \", 18042/total)"
      ],
      "metadata": {
        "id": "Ahk6Wh1TsO1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part (B)\n",
        "\n",
        "(1 pt) Tokenize the tweets. In other words, for each observation, convert the tweet from a single string of running text into a list of individual tokens (possibly with punctuation), splitting on whitespace. The result should be that each observation (tweet) is a list of individual tokens.\n"
      ],
      "metadata": {
        "id": "NhA7sMvTCudK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "# Download the tokenizer\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Create a new column in our DF that contains token lists instead of raw text\n",
        "train_data['tokens'] = train_data['OriginalTweet'].apply(nltk.word_tokenize)\n",
        "\n",
        "test_data['tokens'] = test_data['OriginalTweet'].apply(nltk.word_tokenize)\n",
        "\n",
        "print(train_data['tokens'].head(5))\n",
        "print(test_data['tokens'].head(5))"
      ],
      "metadata": {
        "id": "KRG591BSwlOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part (C)\n",
        "\n",
        "(1 pt) Using a regular expression, remove any URL tokens from each of the observations."
      ],
      "metadata": {
        "id": "XOSFCGp5Cy7B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "##### Remove URL tokens #####\n",
        "\n",
        "def remove_URL_tokens(data, column):\n",
        "  tokens_no_URL = []\n",
        "  for row in data[column]:\n",
        "    tokens_no_URL.append([re.sub(r'http\\S+','', t) for t in row])\n",
        "  data[column] = tokens_no_URL\n"
      ],
      "metadata": {
        "id": "I8miCxxCxBV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URL tokens from testing and training data\n",
        "remove_URL_tokens(test_data, 'tokens')\n",
        "remove_URL_tokens(train_data, 'tokens')\n",
        "\n",
        "print(train_data['tokens'].head(5))\n",
        "print(test_data['tokens'].head(5))"
      ],
      "metadata": {
        "id": "UxAbfYgHGL2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part (D)\n",
        "\n",
        "Remove all punctuation (,.?!;:’\") and special characters(@, #, +, &, =, $, etc). Also, convert all tokens to lowercase only. Can you think of a scenario when you might want to keep some forms of punctuation?"
      ],
      "metadata": {
        "id": "LnmEPJHTC38a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##### Convert tokens into lowercase ####\n",
        "\n",
        "def convert_lowercase(data, column):\n",
        "  lowercase_tokens = []\n",
        "  for row in data[column]:\n",
        "    lowercase_tokens.append([t.lower() for t in row])\n",
        "  data[column] = lowercase_tokens"
      ],
      "metadata": {
        "id": "r7zHrBOKx_gH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert tokens in testing and training data to lowercase\n",
        "convert_lowercase(train_data, 'tokens')\n",
        "convert_lowercase(test_data, 'tokens')\n",
        "\n",
        "print(train_data['tokens'].head(5))\n",
        "print(test_data['tokens'].head(5))"
      ],
      "metadata": {
        "id": "yGoAUHWuGQVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##### Remove punctuation and special characters #####\n",
        "\n",
        "def remove_special_char(data, column):\n",
        "  tokens_no_punct = []\n",
        "  for row in data[column]:\n",
        "    tokens_no_punct.append([re.sub('[^\\w\\s]','', t) for t in row])\n",
        "  data[column] = tokens_no_punct"
      ],
      "metadata": {
        "id": "-DEG9Vp4yIW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove all punctuation and special characters in testing and training data\n",
        "remove_special_char(train_data, 'tokens')\n",
        "remove_special_char(test_data, 'tokens')\n",
        "\n",
        "print(train_data['tokens'].head(5))\n",
        "print(test_data['tokens'].head(5))"
      ],
      "metadata": {
        "id": "sbPWFF-VGmz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### Remove empty tokens ####\n",
        "def remove_empty_tokens(data, column):\n",
        "  tokens_no_empty = []\n",
        "  for row in data[column]:\n",
        "    tokens_no_empty.append([w for w in row if (w != '')])\n",
        "  data[column] = tokens_no_empty"
      ],
      "metadata": {
        "id": "nARZZVp7S4gV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove the empty tokens from the testing and training dataset\n",
        "remove_empty_tokens(train_data, 'tokens')\n",
        "remove_empty_tokens(test_data, 'tokens')\n",
        "\n",
        "print(train_data['tokens'].head(5))\n",
        "print(test_data['tokens'].head(5))"
      ],
      "metadata": {
        "id": "zN1eNn2sG2hP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part (E)\n",
        "\n",
        "Now stem your tokens. This will have the effect of converting similar word forms into identical tokens (e.g. run, runs, running → run). Please specify which stemmer you use."
      ],
      "metadata": {
        "id": "FDmWVPcEC7lQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save current tokens in new row for later (Part K)\n",
        "train_data['tokens_k'] = train_data['tokens']\n",
        "test_data['tokens_k'] = test_data['tokens']"
      ],
      "metadata": {
        "id": "LkJso7CDBEg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### Stemming tokens ####\n",
        "from nltk.stem.porter import *\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def stem_tokens(data, column):\n",
        "  stemmed_tokens = []\n",
        "  for row in data[column]:\n",
        "    stemmed_tokens.append([stemmer.stem(t) for t in row])\n",
        "\n",
        "  data[column] = stemmed_tokens"
      ],
      "metadata": {
        "id": "WzhQMIHKykqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stem the tokens in the training and testing dataset\n",
        "stem_tokens(train_data, 'tokens')\n",
        "stem_tokens(test_data, 'tokens')\n",
        "\n",
        "# Print results\n",
        "print('After stemming:\\n', train_data['tokens'].head(3))\n",
        "print('After stemming:\\n', test_data['tokens'].head(3))"
      ],
      "metadata": {
        "id": "qQ5ZT7WwHKZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part (F)\n",
        "\n",
        "Lastly, remove stopwords. Using the english stopwords list from nltk, remove these common words from your observations. This list is very long (I think almost 200 words), so remove only the first 100 stopwords in the list.\n",
        "\n"
      ],
      "metadata": {
        "id": "KO2x5iJsDBwr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import stopwords\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "# print the top 100 most popular english words\n",
        "sw = stopwords.words('english')[:100]\n",
        "\n",
        "print(sw)"
      ],
      "metadata": {
        "id": "WyN6CouPzEB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##### Remove Stopwords #####\n",
        "def remove_stopwords(data, column):\n",
        "  tokens_no_sw = []\n",
        "  for row in data[column]:\n",
        "    tokens_no_sw.append([w for w in row if w not in sw])\n",
        "  data[column] = tokens_no_sw"
      ],
      "metadata": {
        "id": "U8Bc1hdPz1Pd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove stopwords from the training and testing dataset\n",
        "remove_stopwords(train_data, 'tokens')\n",
        "remove_stopwords(test_data, 'tokens')\n",
        "\n",
        "print(train_data['tokens'].tail(5))\n",
        "print(test_data['tokens'].tail(5))"
      ],
      "metadata": {
        "id": "CIsoMELjHc6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part (G)\n",
        "\n",
        "Now convert your lists of words into vectors of word counts. You may find Scikit-learn’s CountVectorizer useful here. What is the length of your vocabulary?\n"
      ],
      "metadata": {
        "id": "nQB4AlylDGFr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Separate labels from features, converting to numpy arrays\n",
        "X_train, y_train = train_data['tokens'].to_numpy(), train_data['Sentiment'].to_numpy()\n",
        "X_test, y_test = test_data['tokens'].to_numpy(), test_data['Sentiment'].to_numpy()\n",
        "\n",
        "def override_fcn(doc):\n",
        "  # Expect a list of tokens as input\n",
        "  return doc\n",
        "\n",
        "# Count Vectorizer\n",
        "count_vec = CountVectorizer(\n",
        "    analyzer='word',\n",
        "    tokenizer= override_fcn,\n",
        "    preprocessor= override_fcn,\n",
        "    token_pattern= None,\n",
        "    max_features = 1000)\n",
        "\n",
        "# Output is a Scipy Sparse Array\n",
        "counts_train = count_vec.fit_transform(X_train)\n",
        "print(counts_train.toarray())\n",
        "# Use the same words for the testing dataset\n",
        "counts_test = count_vec.transform(X_test)\n",
        "print(counts_test.toarray())\n",
        "\n",
        "# Print the names of each of the features (1000 total))\n",
        "print(\"Length of vocabulary: \", len(count_vec.vocabulary_))\n",
        "# Print this mapping as dictionary\n",
        "print(count_vec.vocabulary_)\n",
        "\n",
        "## Which row represents 'great'\n",
        "print('\\nGreat is located at row: ',count_vec.vocabulary_['great'])"
      ],
      "metadata": {
        "id": "XwfjMKCrzlPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get shape of feature matrix (see number of features)\n",
        "feature_matrix = counts_train.toarray()\n",
        "feature_matrix.shape # (number of tweets, numer of features)"
      ],
      "metadata": {
        "id": "fCNZCdDG07pY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part (H)\n",
        "\n",
        "(4 pts) Recall the definition of the Naive Bayes model. If each document (tweet) is a collection of words (w1, · · · , wN ) belonging to class Ck (k = 0, 1, 2), then the Naive Bayes approach models the probability of each tweet belonging to class k:\n",
        "\n",
        "The last equality follows from our “naive” assumption that words are conditionally independent given class. The probabilities are estimated using the frequencies of words within each class (bag of words), and we assign the class label according to which of the 3 posterior class probabilities (P(Ck|w1,··· ,wN)) is the highest.\n",
        "\n",
        "Fit a Naive Bayes model to your data. Report the training and test error of the model. Use accuracy as the error metric. Also, report the 5 most probable words in each class, along with their counts. You might find Scikit-learn’s MultinomialNB() transformer useful. Use Laplace smoothing to prevent probabilities of zero."
      ],
      "metadata": {
        "id": "v0cVQKcBDYvh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Fit the Naive Bayes model to our training data\n",
        "nb = MultinomialNB()\n",
        "# Fit model to training data (default alpha = 1 has Laplace smoothing)\n",
        "nb.fit(counts_train.toarray(), y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_preds_test = nb.predict(counts_test.toarray())\n",
        "\n",
        "# Predict on train data\n",
        "y_preds_train = nb.predict(counts_train.toarray())\n",
        "\n",
        "\n",
        "print('Test accuracy with simple Naive Bayes:',accuracy_score(y_test,y_preds_test))\n",
        "print('Training accuracy with simple Naive Bayes:',accuracy_score(y_train,y_preds_train))"
      ],
      "metadata": {
        "id": "jEU98a-X1Skm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nb_1 = MultinomialNB(fit_prior=False)\n",
        "# Fit model to training data (default alpha = 1 has Laplace smoothing)\n",
        "nb.fit(counts_train.toarray(), y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_preds_test = nb.predict(counts_test.toarray())"
      ],
      "metadata": {
        "id": "aNTdJ-zqqjCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import *\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_preds_test, \n",
        "                                 pos_label = 1)\n",
        "\n",
        "asthma_auroc = roc_auc_score(y_test, y_preds_test)\n",
        "\n",
        "print(thresholds)\n",
        "plt.plot(fpr,tpr)\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Asthma Model Receiver Operator Characteristic (ROC): ' + str(round(asthma_auroc,5)))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_hJ5wSH4rN-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to get most probable word for a class\n",
        "def most_probable_word_for_class(vectorizer, classifier, classlabel, n):\n",
        "    labelid = list(classifier.classes_).index(classlabel)\n",
        "    feature_names = vectorizer.get_feature_names()\n",
        "    topn = sorted(zip(classifier.feature_count_[classlabel][:], feature_names))[-n:]\n",
        "    for coef, feat in topn:\n",
        "        print(\"word: \", feat, \"count: \", coef)  "
      ],
      "metadata": {
        "id": "YgRNZg5XDQDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Five most probable words for negative sentiment\")\n",
        "most_probable_word_for_class(count_vec, nb, 0, n=5)\n",
        "\n",
        "print(\"Five most probable words for neutral sentiment\")\n",
        "most_probable_word_for_class(count_vec, nb, 1, n=5)\n",
        "\n",
        "print(\"Five most probable words for positive sentiment\")\n",
        "most_probable_word_for_class(count_vec, nb, 2, n=5)"
      ],
      "metadata": {
        "id": "8Nwcx6nLDZHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part (I)\n",
        "\n",
        "(2 pts) Would it be appropriate to fit an ROC curve in this scenario? If yes, explain why. If no, explain why not."
      ],
      "metadata": {
        "id": "_HLXjva2Ddxy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part (J)\n",
        "\n",
        "(2 pts) Redo parts G-H using TF-IDF vectors instead of count vectors. You might find Scikitlearn’s TfidfVectorizer() transformer useful. Report the training and test accuracy. How does this compare to the accuracy using count vectors?"
      ],
      "metadata": {
        "id": "5gUzU8SBDhVq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (J.G)\n",
        "\n",
        "Note the count vectorizer has already been run\n",
        "\n",
        "TfidfVectorizer is equivalent to CountVectorizer followed by TfidfTransformer."
      ],
      "metadata": {
        "id": "UckB864gEE8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "#### TF-IDF Vectorize ####\n",
        "\n",
        "# Note that smoothing is done by default\n",
        "tfidf = TfidfTransformer()\n",
        "\n",
        "tfs_train = tfidf.fit_transform(counts_train)\n",
        "tfs_test = tfidf.transform(counts_test)\n",
        "\n",
        "# Use the TFIDF counts for modelling\n",
        "X_train = tfs_train.toarray()\n",
        "X_test = tfs_test.toarray()\n"
      ],
      "metadata": {
        "id": "RH1ek6zs63Dr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (J.H)"
      ],
      "metadata": {
        "id": "6u17V-ATEGwR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's fit the Naive Bayes model to our training data\n",
        "nb = MultinomialNB()\n",
        "# Fit model to training data\n",
        "nb.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_preds_test = nb.predict(X_test)\n",
        "\n",
        "# Predict on train data\n",
        "y_preds_train = nb.predict(X_train)\n",
        "\n",
        "\n",
        "print('Test accuracy with simple Naive Bayes:',accuracy_score(y_test,y_preds_test))\n",
        "print('Training accuracy with simple Naive Bayes:',accuracy_score(y_train,y_preds_train))"
      ],
      "metadata": {
        "id": "Y00VludB-4O7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part (K)\n",
        "\n",
        "(3 pts) Recall lemmatization converts each word to its base form, which is a bit stronger than simply taking the stem. Redo parts E-H using TF-IDF vectors instead of count vectors. This time use lemmatization instead of stemming. Report train and test accuracy. How does the accuracy with lemmatization compare to the accuracy with stemming?"
      ],
      "metadata": {
        "id": "3xi2So07DmOn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (K.E)"
      ],
      "metadata": {
        "id": "vtD4GppPB3Ze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### Lemmatize ####\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemmatize_tokens(data, column):\n",
        "  lem_tokens = []\n",
        "  for row in data[column]:\n",
        "    lem_tokens.append([lemmatizer.lemmatize(t) for t in row])\n",
        "  data[column] = lem_tokens"
      ],
      "metadata": {
        "id": "orzu1btyAw6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Lemmatize the training and testing data (using the copy of the column we saved from part E)\n",
        "lemmatize_tokens(train_data, 'tokens_k')\n",
        "lemmatize_tokens(test_data, 'tokens_k')\n",
        "\n",
        "print(train_data['tokens_k'].head(5))\n",
        "print(test_data['tokens_k'].head(5))"
      ],
      "metadata": {
        "id": "Cs1wobmMH2b1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (K.F)"
      ],
      "metadata": {
        "id": "z3Ty1p3RB4rX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove stopwords\n",
        "remove_stopwords(train_data, 'tokens_k')\n",
        "remove_stopwords(test_data, 'tokens_k')\n",
        "\n",
        "print(train_data['tokens_k'].tail(5))\n",
        "print(test_data['tokens_k'].tail(5))"
      ],
      "metadata": {
        "id": "JBPCUnRyB2TZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (K.G)"
      ],
      "metadata": {
        "id": "Rn8j-TlQCCgf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Separate labels from features, converting to numpy arrays\n",
        "X_train, y_train = train_data['tokens_k'].to_numpy(), train_data['Sentiment'].to_numpy()\n",
        "X_test, y_test = test_data['tokens_k'].to_numpy(), test_data['Sentiment'].to_numpy()\n",
        "\n",
        "def override_fcn(doc):\n",
        "  # We expect a list of tokens as input\n",
        "  return doc\n",
        "\n",
        "# Count Vectorizer\n",
        "count_vec = CountVectorizer(\n",
        "    analyzer='word',\n",
        "    tokenizer= override_fcn,\n",
        "    preprocessor= override_fcn,\n",
        "    token_pattern= None,\n",
        "    max_features = 1000)\n",
        "\n",
        "# Output is a Scipy Sparse Array\n",
        "counts_train = count_vec.fit_transform(X_train)\n",
        "print(counts_train.toarray())\n",
        "# Use the same words for the testing dataset\n",
        "counts_test = count_vec.transform(X_test)\n",
        "print(counts_test.toarray())\n",
        "\n",
        "# Print the names of each of the features (1000 total))\n",
        "print(count_vec.get_feature_names())\n",
        "# Print this mapping as dictionary\n",
        "print(count_vec.vocabulary_)\n",
        "\n",
        "## Which row represents 'great'\n",
        "print('\\nGreat is located at row: ',count_vec.vocabulary_['great'])"
      ],
      "metadata": {
        "id": "QjhFNaoaB8TT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "#### TF-IDF Vectorize ####\n",
        "\n",
        "# Note that smoothing is done by default\n",
        "tfidf = TfidfTransformer()\n",
        "\n",
        "tfs_train = tfidf.fit_transform(counts_train)\n",
        "tfs_test = tfidf.transform(counts_test)\n",
        "\n",
        "# Use the TFIDF counts for modelling\n",
        "X_train = tfs_train.toarray()\n",
        "X_test = tfs_test.toarray()\n"
      ],
      "metadata": {
        "id": "rFZL-UVqD57h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (K.H)"
      ],
      "metadata": {
        "id": "qTPrOqbGCQZT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the Naive Bayes model to our training data\n",
        "nb = MultinomialNB()\n",
        "# Fit model to training data\n",
        "nb.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_preds_test = nb.predict(X_test)\n",
        "\n",
        "# Predict on train data\n",
        "y_preds_train = nb.predict(X_train)\n",
        "\n",
        "\n",
        "print('Test accuracy with simple Naive Bayes:',accuracy_score(y_test,y_preds_test))\n",
        "print('Training accuracy with simple Naive Bayes:',accuracy_score(y_train,y_preds_train))"
      ],
      "metadata": {
        "id": "XUks09WtEPQk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}